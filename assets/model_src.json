{
    "Qwen1.5":{
        "0.5B-CHAT":{
            "INT8":{
                "url":"https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/resolve/main/qwen1_5-0_5b-chat-q8_0.gguf",
                "output":"models/qwen-1.5-0.5b-chat-q80.gguf",
                "size":"665MB",
                "description":"Q80"
            },
            "INT4":{
                "url":"https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat-GGUF/resolve/main/qwen1_5-0_5b-chat-q4_k_m.gguf",
                "output":"models/qwen-1.5-0.5b-chat-q4k.gguf",
                "size":"407MB",
                "description":"Q4K"
            }
        },
        "1.8B-CHAT":{
            "INT8":{
                "url":"https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat-GGUF/resolve/main/qwen1_5-1_8b-chat-q8_0.gguf",
                "output":"models/qwen-1.5-1.8b-chat-q80.gguf",
                "size":"1.96GB",
                "description":"Q80"
            },
            "INT4":{
                "url":"https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat-GGUF/resolve/main/qwen1_5-1_8b-chat-q4_k_m.gguf",
                "output":"models/qwen-1.5-1.8b-chat-q4k.gguf",
                "size":"1.22GB",
                "description":"Q4K"
            }
        }
    },
    "Qwen2.5":{
        "0.5B-INST":{
            "INT8":{
                "url":"https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q8_0.gguf",
                "output":"models/qwen-2.5-0.5b-inst-q80.gguf",
                "size":"676MB",
                "description":"Q80"
            },
            "INT4":{
                "url":"https://huggingface.co/Qwen/Qwen2.5-0.5B-Instruct-GGUF/resolve/main/qwen2.5-0.5b-instruct-q4_k_m.gguf",
                "output":"models/qwen-2.5-0.5b-inst-q4k.gguf",
                "size":"491MB",
                "description":"Q4K"
            }
        },
        "1.5B-INST":{
            "INT8":{
                "url":"https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q8_0.gguf",
                "output":"models/qwen-2.5-1.5b-inst-q80.gguf",
                "size":"1.89GB",
                "description":"Q80"
            },
            "INT4":{
                "url":"https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct-GGUF/resolve/main/qwen2.5-1.5b-instruct-q4_k_m.gguf",
                "output":"models/qwen-2.5-1.5b-inst-q4k.gguf",
                "size":"1.12GB",
                "description":"Q4K"
            }
        }
    },
    "Qwen3":{
        "0.6B":{
            "INT8":{
                "url":"https://huggingface.co/Qwen/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q8_0.gguf",
                "output":"models/qwen-3-0.6b-q80.gguf",
                "size":"639MB",
                "description":"Q80"
            }
        },
        "1.7B":{
            "INT8":{
                "url":"https://huggingface.co/Qwen/Qwen3-1.7B-GGUF/resolve/main/Qwen3-1.7B-Q8_0.gguf",
                "output":"models/qwen-3-1.7b-q80.gguf",
                "size":"1.83GB",
                "description":"Q80"
            }
        }
    },
    "Llama3.2":{
        "1B-INST":{
            "FP16":{
                "url":"https://huggingface.co/unsloth/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-F16.gguf",
                "output":"models/llama-3.2-1b-inst-fp16.gguf",
                "size":"2.48GB",
                "description":""
            },
            "INT8":{
                "url":"https://huggingface.co/unsloth/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q8_0.gguf",
                "output":"models/llama-3.2-1b-inst-q80.gguf",
                "size":"1.32GB",
                "description":"Q80"
            },
            "INT4":{
                "url":"https://huggingface.co/unsloth/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-1B-Instruct-Q4_K_M.gguf",
                "output":"models/llama-3.2-1b-inst-q4k.gguf",
                "size":"808MB",
                "description":"Q4K"
            }
        },
        "3B-INST":{
            "FP16":{
                "url":"https://huggingface.co/unsloth/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-F16.gguf",
                "output":"models/llama-3.2-3b-inst-fp16.gguf",
                "size":"6.43GB",
                "description":""
            },
            "INT8":{
                "url":"https://huggingface.co/unsloth/Llama-3.2-1B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q8_0.gguf",
                "output":"models/llama-3.2-3b-inst-q80.gguf",
                "size":"3.42GB",
                "description":"Q80"
            },
            "INT4":{
                "url":"https://huggingface.co/unsloth/Llama-3.2-3B-Instruct-GGUF/resolve/main/Llama-3.2-3B-Instruct-Q4_K_M.gguf",
                "output":"models/llama-3.2-3b-inst-q4k.gguf",
                "size":"2.02GB",
                "description":"Q4K"
            }
        }
    }
}